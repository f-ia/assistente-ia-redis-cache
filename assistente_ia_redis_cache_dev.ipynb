{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai requests python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1n4iail6kas",
        "outputId": "22a948f4-e72f-4c1b-b093-8372581feb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting requests\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Installing collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.32.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class AssistenteIAMiniMax:\n",
        "    \"\"\"Assistente IA com MiniMax M2 via HuggingFace - VERS√ÉO CORRIGIDA\"\"\"\n",
        "\n",
        "    def __init__(self, hf_token):\n",
        "        self.token = hf_token\n",
        "        self.historico = []\n",
        "        self.cache = {}\n",
        "\n",
        "        # Configura cliente OpenAI compatible - URL CORRIGIDA\n",
        "        self.client = OpenAI(\n",
        "            base_url=\"https://router.huggingface.co/v1\",  # ‚úÖ URL corrigida\n",
        "            api_key=hf_token,\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ AssistenteIA inicializado com MiniMax M2\")\n",
        "\n",
        "    def consulta_llm(self, prompt, usar_cache=True):\n",
        "        \"\"\"Chama LLM, com cache opcional\"\"\"\n",
        "\n",
        "        # Verifica cache\n",
        "        if usar_cache and prompt in self.cache:\n",
        "            print(f\"üì¶ Resposta do cache: {prompt[:30]}...\")\n",
        "            return self.cache[prompt]\n",
        "\n",
        "        try:\n",
        "            print(f\"‚è≥ Processando: {prompt[:40]}...\")\n",
        "\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=\"MiniMaxAI/MiniMax-M2\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=150,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "\n",
        "            resposta = completion.choices[0].message.content\n",
        "\n",
        "            # Salva em cache\n",
        "            if usar_cache:\n",
        "                self.cache[prompt] = resposta\n",
        "\n",
        "            print(f\"‚úÖ Resposta recebida!\")\n",
        "            return resposta\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao chamar LLM: {str(e)[:100]}\")\n",
        "            return None\n",
        "\n",
        "    def salva_conversacao(self, pergunta, resposta):\n",
        "        \"\"\"Salva pergunta e resposta no hist√≥rico\"\"\"\n",
        "        if resposta is None:\n",
        "            return\n",
        "\n",
        "        self.historico.append({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"pergunta\": pergunta,\n",
        "            \"resposta\": resposta,\n",
        "            \"tokens_aproximados\": len(resposta.split())\n",
        "        })\n",
        "        print(f\"‚úÖ Salvo no hist√≥rico!\")\n",
        "\n",
        "    def exporta_json(self, nome_arquivo=\"conversas.json\"):\n",
        "        \"\"\"Salva hist√≥rico em JSON\"\"\"\n",
        "        with open(nome_arquivo, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.historico, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üíæ Hist√≥rico salvo: {nome_arquivo}\")\n",
        "        return nome_arquivo\n",
        "\n",
        "    def exibe_conversacao(self):\n",
        "        \"\"\"Mostra √∫ltimas conversas\"\"\"\n",
        "        if not self.historico:\n",
        "            print(\"üìã Nenhuma conversa salva ainda!\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìã Total de conversas: {len(self.historico)}\")\n",
        "        for i, item in enumerate(self.historico, 1):\n",
        "            print(f\"\\n{i}. ‚ùì {item['pergunta']}\")\n",
        "            print(f\"   ‚úÖ {item['resposta'][:100]}...\")\n",
        "\n",
        "# ===== EXECUTE DAQUI =====\n",
        "\n",
        "# Seu NOVO token HF (gere um novo ap√≥s revogar o antigo!)\n",
        "HF_TOKEN = \"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\"  # ‚¨ÖÔ∏è SUBSTITUA PELO SEU NOVO TOKEN\n",
        "\n",
        "# Cria assistente\n",
        "assistente = AssistenteIAMiniMax(HF_TOKEN)\n",
        "\n",
        "# Teste 1\n",
        "print(\"\\nüîπ Teste 1: Pergunta sobre IA\")\n",
        "pergunta_1 = \"O que √© aprendizado de m√°quina?\"\n",
        "resposta_1 = assistente.consulta_llm(pergunta_1)\n",
        "if resposta_1:\n",
        "    print(f\"Bot: {resposta_1}\\n\")\n",
        "    assistente.salva_conversacao(pergunta_1, resposta_1)\n",
        "\n",
        "# Teste 2\n",
        "print(\"üîπ Teste 2: Pergunta sobre o Chelsea\")\n",
        "pergunta_2 = \"Qual jogador marcou mais gols pelo Chelsea?\"\n",
        "resposta_2 = assistente.consulta_llm(pergunta_2)\n",
        "if resposta_2:\n",
        "    print(f\"Bot: {resposta_2}\\n\")\n",
        "    assistente.salva_conversacao(pergunta_2, resposta_2)\n",
        "\n",
        "# Teste 3: Cache\n",
        "print(\"üîπ Teste 3: Pergunta repetida (deve vir do cache)\")\n",
        "resposta_3 = assistente.consulta_llm(pergunta_1)\n",
        "if resposta_3:\n",
        "    print(f\"Bot: {resposta_3}\\n\")\n",
        "\n",
        "# Exibe\n",
        "assistente.exibe_conversacao()\n",
        "\n",
        "# Exporta\n",
        "assistente.exporta_json(\"conversas.json\")\n",
        "\n",
        "print(\"\\n‚úÖ Testes conclu√≠dos!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYKG1blR7Lqh",
        "outputId": "5f9c1cdb-6568-493f-fe38-934eb1be5e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AssistenteIA inicializado com MiniMax M2\n",
            "\n",
            "üîπ Teste 1: Pergunta sobre IA\n",
            "‚è≥ Processando: O que √© aprendizado de m√°quina?...\n",
            "‚úÖ Resposta recebida!\n",
            "Bot: <think>\n",
            "I need to answer in Portuguese since the user asked in that language. I want to explain what machine learning is, its key definitions, types, and applications. I should cover models, training, validation, metrics, and algorithms like supervised, unsupervised, and reinforcement learning. \n",
            "\n",
            "I'll also explain the relationship to AI, deep learning, and concepts like overfitting, data splits, and evaluation. I‚Äôll mention generative models, MLOps, and risk factors like bias and privacy. The goal is to be concise yet comprehensive.\n",
            "\n",
            "I want to provide clear definitions and examples related to machine learning. I‚Äôll describe typical steps such as problem framing, dataset acquisition, preprocessing, feature engineering, model selection, training, validation, deployment, and monitoring.\n",
            "\n",
            "I'll also\n",
            "</think>\n",
            "\n",
            "\n",
            "‚úÖ Salvo no hist√≥rico!\n",
            "üîπ Teste 2: Pergunta sobre o Chelsea\n",
            "‚è≥ Processando: Qual jogador marcou mais gols pelo Chels...\n",
            "‚úÖ Resposta recebida!\n",
            "Bot: <think>\n",
            "I‚Äôm considering that Frank Lampard is indeed the top scorer for Chelsea, both overall and in the Premier League. If we look at official stats for their Premier League era from 1992-93 to 2023-24, Lampard has 177 goals. In all competitions, he has 211. However, if we include before the Premier League era, like Peter Osgood's 220 goals from 1966‚Äì73, that could be outdated. Also, I want to make sure that there are no discrepancies in records between official and all-time totals for Chelsea.\n",
            "\n",
            "Chelsea lists Lampard as their top scorer with 211 goals across all competitions, but that‚Äôs not entirely accurate since some top scorers pre-1930 had\n",
            "</think>\n",
            "\n",
            "\n",
            "‚úÖ Salvo no hist√≥rico!\n",
            "üîπ Teste 3: Pergunta repetida (deve vir do cache)\n",
            "üì¶ Resposta do cache: O que √© aprendizado de m√°quina...\n",
            "Bot: <think>\n",
            "I need to answer in Portuguese since the user asked in that language. I want to explain what machine learning is, its key definitions, types, and applications. I should cover models, training, validation, metrics, and algorithms like supervised, unsupervised, and reinforcement learning. \n",
            "\n",
            "I'll also explain the relationship to AI, deep learning, and concepts like overfitting, data splits, and evaluation. I‚Äôll mention generative models, MLOps, and risk factors like bias and privacy. The goal is to be concise yet comprehensive.\n",
            "\n",
            "I want to provide clear definitions and examples related to machine learning. I‚Äôll describe typical steps such as problem framing, dataset acquisition, preprocessing, feature engineering, model selection, training, validation, deployment, and monitoring.\n",
            "\n",
            "I'll also\n",
            "</think>\n",
            "\n",
            "\n",
            "\n",
            "üìã Total de conversas: 2\n",
            "\n",
            "1. ‚ùì O que √© aprendizado de m√°quina?\n",
            "   ‚úÖ <think>\n",
            "I need to answer in Portuguese since the user asked in that language. I want to explain what...\n",
            "\n",
            "2. ‚ùì Qual jogador marcou mais gols pelo Chelsea?\n",
            "   ‚úÖ <think>\n",
            "I‚Äôm considering that Frank Lampard is indeed the top scorer for Chelsea, both overall and in...\n",
            "üíæ Hist√≥rico salvo: conversas.json\n",
            "\n",
            "‚úÖ Testes conclu√≠dos!\n"
          ]
        }
      ]
    }
  ]
}